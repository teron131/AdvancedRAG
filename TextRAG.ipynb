{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet -U pdf2image pytesseract unstructured[all-docs] pillow pydantic lxml pillow matplotlib tiktoken open_clip_torch torch langchain openai chromadb langchain-experimental\n",
    "#! apt install poppler-utils\n",
    "#! apt install tesseract-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and extract elements from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"WildfireStatistics/\"\n",
    "filename = \"WildfireStatistics.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "2024-03-05 07:14:47.549042: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-05 07:14:47.590997: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 07:14:47.591042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 07:14:47.592985: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 07:14:47.603651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 07:14:48.608125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 5\n",
      "Tables: 2\n",
      "Images: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Extract images, tables, and chunk text\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=os.path.join(path, filename),\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    extract_image_block_output_dir=path,\n",
    ")\n",
    "\n",
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "unique_categories = set(category_counts.keys())\n",
    "\n",
    "\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Text count\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(f\"Text: {len(text_elements)}\")\n",
    "\n",
    "# Tables count\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(f\"Tables: {len(table_elements)}\")\n",
    "\n",
    "# Images count\n",
    "image_count = len([name for name in os.listdir(path) if name.endswith(\".jpg\")])\n",
    "print(f\"Images: {image_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate summaries of the texts and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n",
      "\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai.chat_models.base import ChatOpenAI\n",
    "from langchain_together.llms import Together\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "# model = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0)\n",
    "model = Together(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", temperature=0.7, max_tokens=4096)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Apply to texts\n",
    "texts = [i.text for i in text_elements]\n",
    "with get_openai_callback() as callback:\n",
    "    text_summaries = summarize_chain.batch(texts)\n",
    "    print(callback, end=\"\\n\\n\")\n",
    "\n",
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "with get_openai_callback() as callback:\n",
    "    table_summaries = summarize_chain.batch(tables)\n",
    "    print(callback, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text retriever by understanding the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage.in_memory import InMemoryStore\n",
    "from langchain.storage.file_system import LocalFileStore\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# The text_vectorstore to use to index the child chunks\n",
    "text_vectorstore = Chroma(\n",
    "    collection_name=str(path.replace(\"/\", \"_\")) + \"text_vectorstore\",\n",
    "    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=1024),\n",
    "    persist_directory=os.path.join(path, \"text_vectorstore\"),\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = LocalFileStore(os.path.join(path, \"text_docstore\"))\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The text_retriever (empty to start)\n",
    "text_retriever = MultiVectorRetriever(\n",
    "    vectorstore=text_vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    "    # search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [Document(page_content=s, metadata={id_key: doc_ids[i]}) for i, s in enumerate(text_summaries)]\n",
    "text_retriever.vectorstore.add_documents(summary_texts)\n",
    "\n",
    "# Before calling text_retriever.docstore.mset, ensure the texts are encoded to bytes\n",
    "encoded_texts = [(doc_id, text.encode(\"utf-8\")) for doc_id, text in zip(doc_ids, texts)]\n",
    "text_retriever.docstore.mset(encoded_texts)\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [Document(page_content=s, metadata={id_key: table_ids[i]}) for i, s in enumerate(table_summaries)]\n",
    "text_retriever.vectorstore.add_documents(summary_tables)\n",
    "\n",
    "# Before calling text_retriever.docstore.mset, ensure the texts are encoded to bytes\n",
    "encoded_tables = [(doc_id, table.encode(\"utf-8\")) for doc_id, table in zip(doc_ids, tables)]\n",
    "text_retriever.docstore.mset(encoded_tables)\n",
    "\n",
    "text_vectorstore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG chain with ensemble retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_openai.chat_models.base import ChatOpenAI\n",
    "\n",
    "# model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=4096)\n",
    "\n",
    "model = Together(model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", temperature=0.7, max_tokens=4096)\n",
    "\n",
    "# ensemble_retriever = EnsembleRetriever(retrievers=[text_retriever, image_retriever], weights=[0.5, 0.5])\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def decoder(contexts):\n",
    "    \"\"\"Decode a list of text documents from bytes to strings.\"\"\"\n",
    "    return [context.decode(\"utf-8\") for context in contexts]\n",
    "\n",
    "\n",
    "# RAG pipeline\n",
    "chain = {\n",
    "    \"context\": text_retriever | RunnableLambda(decoder),  # | RunnableLambda(split_image_text_types),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableParallel(\n",
    "    {\n",
    "        \"response\": prompt | model | StrOutputParser(),\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "\n",
    "\n",
    "def displayRAG(question):\n",
    "    print(\"###QUESTION###\")\n",
    "    print(question, end=\"\\n\\n\")\n",
    "\n",
    "    with get_openai_callback() as callback:\n",
    "        response = chain.invoke(question)\n",
    "        print(callback, end=\"\\n\\n\")\n",
    "\n",
    "    print(\"###ANSWER###\")\n",
    "    print(response[\"response\"], end=\"\\n\\n\")\n",
    "\n",
    "    # for i, image in enumerate(response[\"context\"][\"images\"]):\n",
    "    # print(f\"###IMAGE{i+1}###\")\n",
    "    # plt_img_base64(image)\n",
    "\n",
    "    for i, text in enumerate(response[\"context\"]):\n",
    "        print(f\"###TEXT{i+1}###\")\n",
    "        print(text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###QUESTION###\n",
      "Wildfire\n",
      "\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n",
      "\n",
      "###ANSWER###\n",
      "\n",
      "Based on the context, what does the Federal 2018 Number of Fires (thousands) indicate?\n",
      "\n",
      "Answer: The Federal 2018 Number of Fires (thousands) indicates that there were 12.5 thousand fires on federal lands in the year 2018.\n",
      "\n",
      "###TEXT1###\n",
      "2018 2019 2020 Number of Fires (thousands) Federal 12.5 10.9 14.4 FS 5.6 5.3 6.7 DOI 7.0 5.3 7.6 2021 14.0 6.2 7.6 2022 11.7 5.9 5.8 Other 0.1 0.2 <0.1 0.2 0.1 Nonfederal 45.6 39.6 44.6 45.0 57.2 Total 58.1 Acres Burned (millions) Federal 4.6 FS 2.3 DOI 2.3 50.5 3.1 0.6 2.3 59.0 7.1 4.8 2.3 59.0 5.2 4.1 1.0 69.0 4.0 1.9 2.1 Other <0.1 <0.1 <0.1 <0.1 Nonfederal 4.1 1.6 3.1 1.9 Total 8.8 4.7 10.1 7.1 <0.1 3.6 7.6\n",
      "\n",
      "###TEXT2###\n",
      "Source: National Interagency Coordination Center (NICC) Wildland Fire Summary and Statistics annual reports. Notes: FS = Forest Service; DOI = Department of the Interior. Column totals may not sum precisely due to rounding.\n",
      "\n",
      "Year Number of Fires Acres burned (millions) 2015 2020 2017 2006 2007\n",
      "\n",
      "Source: NICC Wildland Fire Summary and Statistics annual reports. Note: Number of fires in thousands.\n",
      "\n",
      "The number of fires and acreage burned are indicators of the annual level of wildfire activity. These numbers may not be indicative of fire’s impact on human development or communities, since many fires occur in large, relatively undeveloped areas. Acreage burned also does not indicate the severity of a wildfire, the degree of impact upon forests or soils, or other ecological effects.\n",
      "\n",
      "Most wildfires are human-caused (89% of the average number of wildfires from 2018 to 2022). Wildfires caused by lightning tend to be slightly larger and to burn more acreage (53% of the average acreage burned from 2018 to 2022) than human-caused fires.\n",
      "\n",
      "https://crsreports.congress.gov\n",
      "\n",
      "Wildfire Statistics\n",
      "\n",
      "In 2022, 52% of the nationwide acreage burned by wildfires was on federal lands (4.0 million acres; see Table 1), lower than the 10-year average (64%) of impacted federal land acreage. The other 48% of the acreage burned in 2022 was on state, local, or privately owned lands, though the fires on these lands accounted for 83% of total fires. Of the federal acreage burned nationwide in 2022, 52% (2.1 million acres) burned on DOI land and 47% (1.9 million acres) burned on FS land (see Figure 3). The 2022 figures are driven largely by Alaska, where just over half of the acreage impacted occurred on nonfederal lands (1.6 million acres) and just under half was on DOI lands (1.5 million acres).\n",
      "\n",
      "Resources Another metric useful for assessing wildfire activity is the extent to which nationwide resources—including personnel and equipment—are engaged in wildfire suppression. A proxy for resource commitments is the nationwide Preparedness Level (PL) scale, which ranges from 1 (lowest) to 5 (highest). The higher PLs indicate significant commitment of shared resources. In 2022, the nationwide level was 4 for 10 days and never reached the highest level (5). In contrast, the highest level was reached for 68 days in 2021, the longest since at least 2000.\n",
      "\n",
      "Figure 3. Percentage Acreage Burned by Ownership\n",
      "\n",
      "100% ° Non- ! federal 75% Other [ federal 50% DOI 25% Forest 0% Service 2018 2019 2021 2022 2020\n",
      "\n",
      "Wildfire Damages Wildfires may affect certain ecological resources beneficially, but wildfires also may have devastating impacts, especially for communities affected by wildfire activity. Statistics showing the level of destruction a wildfire caused can provide useful metrics to evaluate the fire’s effect. Such statistics may include acres burned or impacted, lives lost (firefighters and civilians), and structures (residential, commercial, and other) destroyed. For example, in 2022, over 2,700 structures were burned in wildfires; the majority of the damage occurred in California (see Table 2).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displayRAG(\"Wildfire\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
